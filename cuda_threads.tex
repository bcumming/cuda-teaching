% CHAPTER SLIDE
\cscschapter{Going Parallel : Kernels and Threads}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Threads and kernels}
        \begin{itemize}
            \item \emph{threads} are streams of execution, run simultaneously on GPU (1000s)
            \item \emph{kernel} is the task run by each thread
            \item CUDA provides language support for
            \begin{itemize}
                \item writing kernels
                \item launching many threads to execute a kernel in parallel
            \end{itemize}
            \item CUDA hides the low-level details of launching threads
        \end{itemize}
    \end{info}
    \begin{info}{The process for porting to CUDA}
        \begin{enumerate}
            \item formulate algorithm in terms of parallel work items
            \item write a kernel implementing a work item on one thread
            \item launch the kernel with the required number of threads
        \end{enumerate}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Scaled Vector Addition (\axpy)}
        The exercise in the first section used CUBLAS to perform scaled vector addition
            $$\mathbf{y} = \mathbf{y} + \alpha \mathbf{x}$$
            \vspace{-15pt}
        \begin{itemize}
            \item $\mathbf{x}$ and $\mathbf{y}$ are vectors of length $n$ \hfill $x,y \in \mathbb{R}^n$
            \item $\alpha$ is scalar \hfill $\alpha\in\mathbb{R}$
        \end{itemize}
        \axpy can be performed as $n$ \emph{independent} operations
        $$y_i \leftarrow y_i + a*x_i,\quad i = {0, 1, \dots, n-1}$$
        which can be performed independently and in any order
    \end{info}

    \begin{code}{\axpy implemented with for loop}
%..................................
        \begin{lstlisting}[style=boxcuda]
void axpy(double *y, double *x, double a, int n) {
  for(int i=0; i<n; ++i)
    y[i] = y[i] + a*x[i];
}
        \end{lstlisting}
%..................................
    \end{code}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{What is a kernel?}
    \begin{itemize}
        \item a kernel defines the work item for a single thread
        \item the work is performed by many threads executing the same kernel \emph{simultaneously}
        \item Conceptually corresponds to the inner part of a loop for BLAS1 operations like \axpy
    \end{itemize}
    \end{info}

    \vspace{-10pt}
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{codecolumn}{host : add two vectors}
%..................................
        \begin{lstlisting}[style=boxcudatiny]

void add_cpu(int *a, int *b, int n){
  for(auto i=0; i<n; ++i)
    a[i] = a[i] + b[i];
}
        \end{lstlisting}
%..................................
            \end{codecolumn}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{codecolumn}{CUDA : add two vectors}
%..................................
        \begin{lstlisting}[style=boxcudatiny]
__global__
void add_gpu(int *a, int *b, int n){
  auto i = threadIdx.x;
  a[i] = a[i] + b[i];
}
        \end{lstlisting}
%..................................
            \end{codecolumn}
        \end{column}
    \end{columns}

    \vspace{-2pt}
    \begin{info}{}
    \begin{itemize}
        \item \lst{__global__} keyword indicates a kernel
        \item \lst{threadIdx} used to find unique id of each thread
    \end{itemize}
    \end{info}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{launching a kernel}
    \begin{itemize}
        \item host code launches a kernel on the GPU \emph{asyncronously}
        \item CUDA provides special \lst{@<<<@_,_@>>>@} syntax for launching a kernel
        \begin{itemize}
            \item \lst{add_gpu@<<<@1, num_threads@>>>@(args... )} will launch the kernel \lst{add_gpu} with \lst{num_threads} parallel threads.
        \end{itemize}
    \end{itemize}
    \end{info}

    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{codecolumn}{host : add two vectors}
%..................................
        \begin{lstlisting}[style=boxcuda]
auto n = 1024;
auto a = host_malloc<int>(n);
auto b = host_malloc<int>(n);
add_cpu(a, b, n);
        \end{lstlisting}
%..................................
            \end{codecolumn}
        \end{column} \begin{column}{0.5\textwidth}
            \begin{codecolumn}{CUDA : add two vectors}
%..................................
        \begin{lstlisting}[style=boxcuda]
auto n = 1024;
auto a = device_malloc<int>(n);
auto b = device_malloc<int>(n);
add_gpu@<<<@1,n@>>>@(a, b, n);
        \end{lstlisting}
%..................................
            \end{codecolumn}
        \end{column}
    \end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exercise: My First Kernel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Open \lst{topics/cuda/practicals/axpy/axpy_kernel.cu}

    \begin{enumerate}
        \item Write a kernel that implements \axpy for \lst{double}
        \begin{itemize}
            \item \lst{axpy_kernel(double *y, double *x, double a, int n)}
            \item \extra can you write a C++ templated version for any type?
        \end{itemize}

    \item launch the kernel (look for \lst{TODO})
        \item Compile the test and run
        \begin{itemize}
            \item it will pass with no errors on success
            \item first try with small vectors of size 8
            \item try increasing launch size... what happens?
        \end{itemize}
        \item \extra can you extend the kernel to work for larger arrays?
    \end{enumerate}
\end{frame}


