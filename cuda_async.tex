%++++++++++++++++++++++++++++++++
\cscschapter{Managing Asynchronous Work}
%++++++++++++++++++++++++++++++++

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous execution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Explain that tasks can be launched asynchronously

    benefits:
    \begin{itemize}
        \item host can continue work while GPU works
        \item multiple tasks can be run on GPU simultaneously
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous execution example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{codecolumn}{host code}
                \begin{lstlisting}[style=boxcudatiny]
kernel_1<<<...>>>(...);
kernel_2<<<...>>>(...);
host_1(...);
host_2(...);
                \end{lstlisting}
            \end{codecolumn}
        The host:
        \begin{itemize}
            \item launches the two CUDA kernels
            \item then executes host calls sequentially 
        \end{itemize}
        The GPU:
        \begin{itemize}
            \item executes asynchronously to host
            \item executes kernels sequentially
        \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            %\begin{info}{execution}
                \includegraphics[width=\textwidth]{./images/async_null.pdf}
            %\end{info}
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{CUDA Support for Asynchronous Execution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    The CUDA language and runtime libraries provide mechanisms for coordinating asynchronous GPU execution

    \begin{itemize}
        \item \emph{CUDA streams} can asynchronously run independent kernels and memory transfers
        \item \emph{CUDA events} can be used to synchronize streams and query the status of kernels and transfers
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Streams}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    CUDA kernel calls and host-device memory transfers can be given to a stream
    \begin{itemize}
        \item kernels in different streams can run asynchronously
        \item kernels in the same stream are executed sequentially
        \item if no stream is specified, all kernels are launched in the default 0 stream
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Streams}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    A stream is represented using a \lst{cudaStream_t} type
    \begin{itemize}
        \item \lst{cudaStreamCreate(cudaStream_t* s)} creates a new stream, and stores the stream id in \lst{s}.
        \item \lst{cudaStreamDestroy(cudaStream_t s)} frees resources for the stream with id \lst{s}.
        \item To launch a kernel on a stream specify the stream id as a fourth parameter to the launch syntax \\ \lst{<<<grid, block, shared_size, stream>>>}
    \end{itemize}

    \begin{code}{basic cuda stream useage}
        \begin{lstlisting}[style=boxcudatiny]
// create stream
cudaStream_t stream;
cudaStreamCreate(&stream);
// launch kernel in stream
my_kernel<<<grid_dim, block_dim, shared_size, stream>>>(..)
// release stream when finished
cudaStreamDestroy(stream);
        \end{lstlisting}
\end{code}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous device execution example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{columns}[T]
        \begin{column}{0.4\textwidth}
            \begin{codecolumn}{host code}
                \begin{lstlisting}[style=boxcudatiny]
kernel_1<<<,,,stream_1>>>();
kernel_2<<<,,,stream_2>>>();
kernel_3<<<,,,stream_1>>>();
                \end{lstlisting}
            \end{codecolumn}
            \begin{itemize}
                \item \lst{kernel_1} and \lst{kernel_2} are serialized in \lst{stream_1}
                \item \lst{kernel_2} can run asynchronously in \lst{stream_2}
            \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            %\begin{info}{execution}
                \includegraphics[width=\textwidth]{./images/async_two_streams.pdf}
            %\end{info}
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Streams : rule of thumb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Ideally you don't want to use streams to fill the device with work.
    \begin{itemize}
        \item a sign that you don't have enough work to start with
        \item not a hard and fast rule
        \item good for ``filling in'' between kernel executions
    \end{itemize}

    Streams come into their own for overlapping communication and computation
    \begin{itemize}
        \item because you can transfer data full speed both directions when kernels are executing on the GPU
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Events}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    CUDA provides events
    \begin{itemize}
        \item synchronize tasks in different streams:
        \begin{itemize}
            \item don't start work in stream A until stream B has finished.
            \item wait until required data has finished copy from host before launching kernel
        \end{itemize}
        \item allow us to query status of concurrent tasks
        \begin{itemize}
            \item has kernel finished/started yet?
            \item how long did a kernel take to compute?
        \end{itemize}
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Events}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    How to create and event

    How to destroy an event

    How to insert an event into a stream

    How to make a stream wait on an event
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Events : timing example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    timing kernel execution on GPU requires events

    code
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Events : copy-kernel synchronization example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    code
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous copy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \lst{cudaMemcpyAsync}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exercises}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{itemize}
        \item time to visit \lst{util.h} and add helpers for asynchronous copy
        \item just give the event and stream wrappers ``as is''
        \begin{itemize}
            \item walk through their use in the memcpy example
        \end{itemize}
        \item fire up nvprof
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous example: streaming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Streaming is a workload where we can break data and work into chunks
    \begin{itemize}
        \item work on one chunk of data while the next chunk is being sent
        \item send each chunk back to host and take next available chunk of work
        \item there has to be enough work in each chunk to hide ...
    \end{itemize}

    Take, for example our axpy example...
    \begin{itemize}
        \item clearly no amount of overlap will help us
        \item note that we get full speed both directions
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Asynchronous example: streaming}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Show Newton kernel : lotsa work
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exercises}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    Overlapping work for Newton
\end{frame}
