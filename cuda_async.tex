%++++++++++++++++++++++++++++++++
\cscschapter{Concurrency}
%++++++++++++++++++++++++++++++++

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Concurrency}
        \emph{Concurrency} is the ability to perform multiple CUDA operations simultaneously
        \begin{itemize}
            \item CUDA kernels
            \item copying from host to device
            \item copying from device to host
            \item operations on the host CPU
        \end{itemize}
    \end{info}

    \begin{info}{Concurrency enables}
        \begin{itemize}
            \item both CPU and GPU can work at the same time
            \item multiple tasks can be run on GPU simultaneously
            \item overlapping of communication and computation
        \end{itemize}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{codecolumn}{Host code}
                \begin{lstlisting}[style=boxcudatiny]
kernel_1@<<<@...@>>>@(...);
kernel_2<@<<.@..@>>>@(...);
host_1(...);
host_2(...);
                \end{lstlisting}
            \end{codecolumn}
            %\begin{info}{CPU and GPU are asynchronous}
                The host:
                \begin{itemize}
                    \item launches the two CUDA kernels
                    \item then executes host calls sequentially 
                \end{itemize}
                The GPU:
                \begin{itemize}
                    \item executes asynchronously to host
                    \item executes kernels sequentially
                \end{itemize}
            %\end{info}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=\textwidth]{./images/async_null.pdf}
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    The CUDA language and runtime libraries provide mechanisms for coordinating asynchronous GPU execution

    \begin{itemize}
        \item \emph{CUDA streams} can concurrently run independent kernels and memory transfers
        \item \emph{CUDA events} can be used to synchronize streams and query the status of kernels and transfers
    \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Streams}
        A CUDA stream is a sequence of operations that execute in \emph{issue order} on the GPU
        \begin{itemize}
            \item CUDA operations are kernels and copies between host and device memory spaces
        \end{itemize}
    \end{info}

    \begin{info}{Streams and concurrency}
        \begin{itemize}
            \item operations in different streams \emph{may} run concurrently
            \begin{itemize}
                \item there have to be sufficient resources on the GPU (registers, shared memory, blocks, etc)
            \end{itemize}
            \item operations in the same stream \emph{are} executed sequentially
            \item if no stream is specified, all kernels are launched in the default stream
        \end{itemize}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Managing streams}
        A stream is represented using a \lst{cudaStream_t} type
        \begin{itemize}
            \item \lst{cudaStreamCreate(cudaStream_t* s)} and \lst{cudaStreamDestroy(cudaStream_t s)} can be used to create and free CUDA streams respectively
            \item To launch a kernel on a stream specify the stream id as a fourth parameter to the launch syntax \\
                \begin{center} \lst{kernel@<<<@grid_dim, block_dim, shared_size, stream@>>>@(...)} \end{center}
            \item the default CUDA stream is the \lst{NULL} stream, or stream 0 (\lst{cudaStream_t} is an integer)
        \end{itemize}
    \end{info}

    \begin{code}{Basic cuda stream usage}
        \begin{lstlisting}[style=boxcudatiny]
// create stream
cudaStream_t stream;
cudaStreamCreate(&stream);
// launch kernel in stream
my_kernel@<<<@grid_dim, block_dim, shared_size, stream@>>>@(..)
// release stream when finished
cudaStreamDestroy(stream);
        \end{lstlisting}
\end{code}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{columns}[T]
        \begin{column}{0.45\textwidth}
            \begin{codecolumn}{Host code}
                \begin{lstlisting}[style=boxcudatiny]
kernel_1@<<<@,,,stream_1@>>>@();
kernel_2@<<<@,,,stream_2@>>>@();
kernel_3@<<<@,,,stream_1@>>>@();
                \end{lstlisting}
            \end{codecolumn}
            \begin{itemize}
                \item \footnotesize \lst{kernel_1} and \lst{kernel_3} are serialized in \lst{stream_1}
                \item \lst{kernel_2} can run asynchronously in \lst{stream_2}
                \item note that \lst{kernel_2} will only run concurrently if there are sufficient resources available on the GPU, i.e. if \lst{kernel_1} is not using all of the SMXs.
            \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            \includegraphics[width=\textwidth]{./images/async_two_streams.pdf}
        \end{column}
    \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \begin{info}{Asynchronous copy}
        \centering \lst{cudaMemcpyAsync(*dst, *src, size, kind, cudaStream_t stream = 0);}
        \begin{itemize}
            \item takes an additional parameter stream, which is 0 by default
            \item returns immediately after initiating copy
            \begin{itemize}
                \item host can do work while copy is performed
                \item only if \emph{pinned memory} is used
            \end{itemize}
            \item copies in the same direction (i.e. H2D or D2H) are serialized
            \begin{itemize}
                \item copies in opposite directions are concurrent if in different streams
            \end{itemize}
        \end{itemize}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{What is pinned memory?}
        Pinned memory (or page-locked) memory will not be paged out to disk when memory runs low
        \begin{itemize}
            \item the GPU can safely remotely read/write the memory directly without host involvement
            \item only use for transfers, because it easy to run out of memory
        \end{itemize}
    \end{info}

    \begin{info}{Managing pinned memory}
        \centering \lst{cudaMallocHost(**ptr, size);} and \lst{cudaFreeHost(*ptr);}
        \begin{itemize}
            \item allocate and free pinned memory (\lst{size} is in bytes).
        \end{itemize}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Asynchronous copy example: streaming workloads}
        Computations that can be performed independently, e.g. our \axpy example:
        \begin{itemize}
            \item data in host memory has to be copied to the device, and the result copied back after the kernel is computed.
            \item we can overlap the copies with the kernel calls by breaking the data into chunks.
        \end{itemize}
    \end{info}
    \includegraphics[width=\textwidth]{./images/overlap.pdf}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{CUDA events}
        To implement the streaming workload we have to coordinate operations on the GPU.
        CUDA events can be used for this purpose.
        \begin{itemize}
            \item synchronize tasks in different streams, e.g.:
            \begin{itemize}
                \item don't start kernel in kernel stream until data copy stream has finished.
                \item wait until required data has finished copy from host before launching kernel
            \end{itemize}
            \item query status of concurrent tasks
            \begin{itemize}
                \item has kernel finished/started yet?
                \item how long did a kernel take to compute?
            \end{itemize}
        \end{itemize}
    \end{info}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Managing events}
        \lst{cudaEventCreate(cudaEvent_t*);} and \lst{cudaEventDestroy(cudaEvent_t);}
            \begin{itemize}
                \item create and free \lst{cudaEvent_t}
            \end{itemize}
        \lst{cudaEventRecord(cudaEvent_t, cudaStream_t);}
            \begin{itemize}
                \item enqueue an event in a stream
            \end{itemize}
        \lst{cudaEventSynchronize(cudaEvent_t);}
            \begin{itemize}
                \item make host execution wait for event to occur.
            \end{itemize}
        \lst{cudaEventQuery(cudaEvent_t)}
            \begin{itemize}
                \item test if the work before an event in a queue has been completed
            \end{itemize}
        \lst{cudaEventElapsedTime(float*, cudaEvent_t, cudaEvent_t);}
            \begin{itemize}
                \item get time between two events
            \end{itemize}
    \end{info}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{code}{Using events to time kernel execution}
        \begin{lstlisting}[style=boxcudatiny]
cudaEvent_t start, end;
cudaStream_t stream;
float time_taken;

// initialize the events and streams
cudaEventCreate(&start);
cudaEventCreate(&end);
cudaStreamCreate(&stream);

cudaEventRecord(start, stream); // enqueue start in stream
my_kernel@<<<@grid_dim, block_dim, 0, stream@>>>@();
cudaEventRecord(end, stream);   // enqueue end in stream
cudaEventSynchronize(end);      // wait for end to be reached
cudaEventElapsedTime(&time_taken, start, end);

std::cout << "kernel took " << 1000*time_taken << " s\n";

// free resources for events and streams
cudaEventDestroy(start);
cudaEventDestroy(end);
cudaStreamDestroy(stream);
        \end{lstlisting}
    \end{code}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{code}{Copy$\rightarrow$kernel synchronization}
        \begin{lstlisting}[style=boxcudatiny]
cudaEvent_t event;
cudaStream_t kernel_stream, h2d_stream;
size_t size = 100*sizeof(double);
double *dptr, *hptr;

// initialize
cudaEventCreate(&event);
cudaStreamCreate(&kernel_stream);
cudaStreamCreate(&h2d_stream);

cudaMalloc(&dptr, size);
cudaMallocHost(&hptr, size); // use pinned memory!

// start asynchronous copy in h2d_stream
cudaMemcpyAsync(dptr, hptr, size,
                cudaMemcpyHostToDevice, h2d_stream);
// enqueue event in stream
cudaEventRecord(event, h2d_stream);
// make kernel_stream wait for copy to finish
cudaStreamWaitEvent(kernel_stream, event, 0);
// enqueue my_kernel to start when event has finished
my_kernel@<<<@grid_dim, block_dim, 0, kernel_stream@>>>@();

// free resources for events and streams
cudaEventDestroy(event);
cudaStreamDestroy(h2d_stream);
cudaStreamDestroy(kernel_stream);
cudaFree(dptr);
cudaFreeHost(hptr);
        \end{lstlisting}
    \end{code}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exercises}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{enumerate}
        \item Open \lst{util.h} in \lst{cuda/examples/async} and understand \lst{copy_to_@\{@host@/@device@\}@_async()} and \lst{malloc_pinned_host()}

        \item Open \lst{CudaEvent.h} and \lst{CudaStream.h}
        \begin{itemize}
            \item what is the purpose of these classes?
            \item what does \lst{CudaStream::enqueue_event()} do?
        \end{itemize}

        \item Open \lst{memcopy1.cu} and run
        \begin{itemize}
            \item what does the benchmark test?
            \item what is the effect of turning on \lst{USE_PINNED}?\\Hint: try small and large values for \lst{n} (8, 16, 20, 24)
        \end{itemize}

        \item Inspect \lst{memcopy2.cu} and run
        \begin{itemize}
            \item what effect does changing the number of chunks have?
        \end{itemize}

        \item Inspect \lst{memcopy3.cu} and run
        \begin{itemize}
            \item how does it differ from \lst{memcopy2.cu}?
            \item what effect does changing the number of chunks have?
        \end{itemize}
    \end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{code}{Using events to time kernel execution : \textbf{with helpers}}
        \begin{lstlisting}[style=boxcudatiny]
CudaStream stream(true);

auto start = stream.enqueue_event();
my_kernel@<<<@grid_dim, block_dim, 0, stream.stream()@>>>@();
auto end = stream.enqueue_event();
end.wait();
auto time_taken = end.time_since(start);

std::cout << "kernel took " << 1000*time_taken << " s\n";
        \end{lstlisting}
    \end{code}
    \begin{code}{Copy$\rightarrow$kernel synchronization : \textbf{with helpers}}
        \begin{lstlisting}[style=boxcudatiny]
CudaStream kernel_stream(true), h2d_stream(true);
auto size = 100;
auto dptr = device_malloc<double>(size);
auto hptr = pinned_malloc<double>(size);

copy_to_device_async<double>(hptr,dptr,size,h2d_stream.stream());
auto event = h2d_stream.enqueue_event();
kernel_stream.wait_on_event(event);
my_kernel@<<<@grid_dim, block_dim, 0, kernel_stream.stream()@>>>@();

cudaFree(dptr);
cudaFreeHost(hptr);
        \end{lstlisting}
    \end{code}
\end{frame}

%++++++++++++++++++++++++++++++++
\cscschapter{Profiling CUDA}
%++++++++++++++++++++++++++++++++

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Profiling CUDA applications}
        To analyze concurrent applications we need tools that can visually represent application flow.
        \\
        The CUDA toolkit provides the tools \emph{nvprof} and \emph{nvvp} for profiling our GPU applications
        \begin{itemize}
            \item there are visual tools for Windows and Eclipse too
            \item they work for OpenACC applications too
        \end{itemize}
    \end{info}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{nvprof}
        \emph{nvprof} is a command line tool
        \begin{itemize}
            \item can be used to generate text reports
            \item \lst{nvprof --help} for a full list of options
            \item \lst{nvprof app.exe} will perform basic profling of application and print text summary
            \item \lst{nvprof -o profile.out app.exe} will save profile information to file \lst{profile.out} for visualization with nvvp
        \end{itemize}
    \end{info}

    \begin{info}{Demonstration}
        Use nvprof on the memcopy test codes
    \end{info}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{nvvp}
        \emph{nvvp} is a graphical tool for visualizing CUDA applications
        \begin{itemize}
            \item can also be used to perform interactive profiling and guided analysis
            \item this is not so easy on Cray systems
            \item we can also use the output from nvprof
            \begin{itemize}
                \item use \lst{nvprof -o profile.out ... ./app.out} to generate detailed analysis
                \item this can take a long time, because each kernel has to be replayed multiple times to collect all of the information required for the report.
            \end{itemize}
        \end{itemize}
    \end{info}

    \begin{info}{Demonstration}
        Use nvvp on the output of nvprof for the memcopy examples
    \end{info}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{info}{Some rough guidelines for concurrency}
        Ideally for most workloads you don't want to rely on streams to fill the GPU with work
        \begin{itemize}
            \item a sign that the working set per GPU is not large enough
            \item full concurrency is difficult in practice
            \begin{itemize}
                \item a low-level optimization strategy for the last few \%
            \end{itemize}
            \item this isn't a hard and fast rule
        \end{itemize}

        Streams come into their own for overlapping communication and computation
        \begin{itemize}
            \item possible to transfer data in both directions concurrently with kernels execution
        \end{itemize}
    \end{info}
\end{frame}

